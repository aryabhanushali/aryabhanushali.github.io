<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Structured AI Output with Pydantic & Tool Calling</title>
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <nav>
        <ul>
            <li><a href="../index.html#home">Home</a></li>
            <li><a href="../index.html#about">About Me</a></li>
            <li><a href="../index.html#projects">Projects</a></li>
            <li><a href="../index.html#blog">Blog</a></li>
        </ul>
    </nav>
    <main class="section active">
        <h1>Structured AI Output with Pydantic & Tool Calling</h1>
        <p class="blog-meta">May 27, 2025 • Arya Bhanushali • AI Tooling, Pydantic, LLMs</p>
        <article>

            <p>
                My first week at Elevance Health has been a crash course in building AI-integrated systems. I’ve been working on tools that rely on structured outputs from large language models, and it’s given me a new appreciation for how cleanly AI can integrate into backend logic when the right structure is in place. This post covers three key technologies I focused on: <strong>Pydantic</strong>, <strong>OpenAI’s tool calling</strong>, and <strong>structured model output</strong>. This experience has been unique in the sense that I work within the context of the company with its prexisting workflows as well as securities.
            </p>

            <p><strong>Pydantic for Data Modeling</strong></p>
            <p>
                Pydantic is a Python library that lets you define data models with validation baked in. It’s been used for parsing responses and enforcing constraints without manually checking types or null values.
            </p>



            <p>
                With this, I can trust the data structure at runtime—and when something doesn’t fit, Pydantic lets me know immediately.
            </p>

            <p><strong>Calling Tools with OpenAI</strong></p>
            <p>
                OpenAI’s tool calling API introduces a pattern where language models don’t just respond—they invoke functions. When a user asks a question like “Can you check this pdf?”, the model can respond with a structured function call.
            </p>


            <p>
                I’ve been using this to connect the model with backend logic—like flagging finance anomalies or comparing contract terms. The model figures out what needs to be done and passes the right inputs, all without requiring custom parsing or prompt hacking.
            </p>

            <p><strong>Structured JSON Output from LLMs</strong></p>
            <p>
                Another feature I’ve found powerful is OpenAI’s <code>response_format="json"</code>. When enabled, the model outputs pure JSON, which plugs directly into the rest of the pipeline.
            </p>


            <p>
                This level of structure makes it possible to build applications where the model output is both machine-readable and action-ready—ideal for automation workflows.
            </p>

            <p><strong>Takeaways</strong></p>
            <p>
                This week has reshaped how I think about LLMs. They’re not just for generating text—they’re components in larger systems, capable of reasoning, formatting, and interacting through well-defined interfaces. With Pydantic ensuring data integrity, tool calling driving logic, and structured outputs enabling automation, I’m excited to keep exploring how these systems can scale in real-world environments.
            </p>

        </article>
        <a href="../index.html#blog" class="back-to-blog">← Back to Blog</a>
    </main>
</body>
</html>
